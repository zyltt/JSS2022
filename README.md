# Deep Learning Framework Testing via Hierarchical and Heuristic Model Generation

This is the implementation repository of our *JSS'22* paper: **Deep Learning Framework Testing via Hierarchical and Heuristic Model Generation**.





## 1. Description

Deep learning frameworks are the foundation of deep learning model construction and inference. Many testing methods using deep learning models as test inputs are proposed to ensure the quality of deep learning frameworks. However, there are still critical challenges in the model generation, model instantiation, and result analysis. To bridge the gap, we propose Ramos, a hierarchical heuristic deep learning framework testing method. To generate diversified models, we design a novel hierarchical structure to represent the building block of the model. Based on this structure, new models are generated by the mutation method. To trigger more precision bugs in deep learning frameworks, we design a heuristic method to increase the error triggered by models and guide the subsequent model generation. To reduce false positives, we propose an API mapping rule between different frameworks to aid model instantiation. Further, we design different test oracles for crashes and precision bugs respectively. We conduct experiments under three widely-used frameworks(TensorFlow, PyTorch, and MindSpore) to evaluate the effectiveness of Ramos. The results show that Ramos can effectively generate diversified models and detect more deep learning framework bugs, including crashes and precision bugs, with fewer false positives. Additionally, 14 of 15 are confirmed by developers.



You can access this repository using the following command:

```shell
git clone git@github.com:zyltt/JSS2022.git
```



## 2. Framework version

We use three widely-used DL frameworks (*i.e.*, ***TensorFlow***, ***PyTorch***, and ***MindSpore***). We conduct the experimental environment as follow:

| TensorFlow | PyTorch | MindSpore | Keras |
| :--------: | :-----: | :-------: | :---: |
|   2.6.0    | 1.12.0  |   1.7.0   | 2.6.0 |

If you don't want to reproduce the experiments, you can directly get the output from the output directory.



## 3. Datasets(Optional)

Our approach is not sensitive to datasets, *i.e.*, theoretically any data type can be used for testing.  So you can just test with **randomly generated dataset** with our source code.  

If you want to do comparative experiments with existing approaches, 6 widely-used datasets mentioned in our paper can be used, *i.e.*, **MNIST**, **F-MNIST**, **CIFAR-10**, **ImageNet**, **Sine-Wave** and **Stock-Price**. The first three ones can be accessed by [Keras API](https://keras.io/api/datasets/)，while the rest can be access from [OneDrive](https://onedrive.live.com/?authkey=%21ANVR8C2wSN1Rb9M&id=34CB15091B189D3E%211909&cid=34CB15091B189D3E)(`dataset.zip`) provided by [LEMON](https://github.com/Jacob-yen/LEMON).



## 4. Environment

**Step 0:** Please install ***anaconda***.

**Step 1:** Create a conda environment. Run the following commands.

```sh
conda create -n Ramos python=3.9
source activate Ramos
pip install tensorflow==2.6.0
pip install mindspore==1.7.0
pip install torch==1.12.0
pip install keras==2.6.0
```

## 5. File structure

This project contains four folders. The **LEMON-master** folder is the downloaded open source code for LEMON. The **Muffin-main** folder is the downloaded open source code for Muffin. The **Ramos** folder is the source code for our method. The **result** folder is the experimental result data. To know the execution methods of Muffin and LEMON, please refer to the corresponding research papers. In this document, we will introduce how to run the source code for Ramos.

### 5.1. File structure of the source code for Ramos

In the source code for Ramos, the **Dataset** folder is the required dataset. The folders named **DataStruct, Test, and Method** contain the body for the method. The configuration file **globalConfig.py** is in the **DataStruct** folder You can set the parameters of Ramos by modifying the parameters in **globalConfig.py**. The program entry of the method is **main.py**. Run **main.py** to run Ramos after completing parameter configuration.

The other files are used in the Evaluation section. We will explain how to use these files one by one in the following sections.

### 5.2. File structure of the experimental result

The **Cradle_result**,  **Lemon_result**, and **Muffin_result** folders contain the errors of these models under each dataset, the structural analysis of the model, and the string representation of the model. The **Muffin_logs** folder contains the log of crashes detected by Muffin. The **Ramos_crash_logs** folder contains logs of crashes detected by Ramos. The **Ramos_result_example** folder contains examples of Ramos execution results. The folder **Ramos_strategy_pair_1** and **Ramos_strategy_pair_2** are the results of Ramos execution under different strategy pairs and datasets. Especially, **Ramos_strategy_pair_1** contains three Mutation Modes and three Feedback Tendencies.

## 6. Experiments

Make sure you are now in the ***conda*** visual environment!

### 6.1 Main method

#### 1. Configuration

A configuration file in the **DataStruct** directory **globalConfig.py** should be provided to flexibly set up testing configuration. Here is an example:

```python
# coding=utf-8

import numpy as np
from DataStruct.population import Population
from DataStruct.genetypeQueue import GenetypeQueue
class GlobalConfig:
    fail_time = 0 
    N = 0  
    L = 2  
    operatorNum = np.array([4,1])  
    pointNum = [4,4]  
    c0 = 3 
    flatOperatorMaps = []  
    resultNum = 3  
    resGenetype = []  
    maxMutateTime = 10000 
    P = Population()  
    Q = GenetypeQueue()  
    error_cal_mode = "max"
    initMutateTime = 200 
    final_module = [] 
    channels = []
    dataset = 'imagenet'
    h = 48 
    w = 48 
    batch = 10 
    k = 1 
    mode = 2 
    writer = None 
    basicOps = ['identity','None','1*1','depthwise_conv2D','separable_conv2D','max_pooling2D',
                'average_pooling2D','conv2D','conv2D_transpose','ReLU','sigmoid','tanh','leakyReLU',
                'PReLU','ELU','batch_normalization']
    basicWeights = [1]*len(basicOps) 
    basicProp = 0.2 
```

* `fail_time` is the total number of model build failures, with an initial value of 0.

* `N` indicates the size of the population, with an initial value of 0.

* `L` indicates the number of layers in the hierarchical structure, the available options include 2 and 3.

* `operatorNum` indicates the number of motifs in each layers in the hierarchical structure.

* `pointNum` indicates the number of vertexs in each motif in the hierarchical structure.

* `batch,c0,h,w` indicate the shape of the input tensor. They are only used in the random generated dataset.

* `flatOperatorMaps,resultNum,resGenetype,P,Q,error_cal_mode,final_module,channels,writer,basicOps,basicWeights` are intermediate variables of the method and should not be modified.

* `dataset` indicates the name of dataset. The available options include `random`, `cifar10`, `mnist`, `fashion_mnist`, `imagenet`, `sinewave`, `price`.

* `maxMutateTime` indicates the total number of mutations.

* `initMutateTime` indicates the total number of mutations in the initialization stage.

* `k` is the order of the tournament algorithm.

* `mode` indicates the strategies of feedback mode. The available options include: 

  ***0***: **Feedback1.** Only primitive operators are fed back.

  ***1***: **Feedback2.** Only composite operators are fed back. 

  ***2***: **Feedback3.** Both primitive operators and composite operators are fed back. 

* `basicProp` indicates the probability of selecting the basic operator. The available options include 0.2, 0.5 and 0.8.

#### 2. Preprocessing

**Dataset:** Set **dataset_name** in **/Dataset/get_dataset.py**. **dataset_name** can be chosen from `cifar10`、 mnist、fashion_mnist、imagenet、sinewave、price. Then, execute the following command in `/Dataset` to preprocess the dataset or downloading them from [Keras API](https://keras.io/api/datasets/) if you want to use existing dataset:

```shell
python get_dataset.py
```

#### 3. Start

Use the following command to run the experiment according to the configuration:

```shell
python main.py
```

The testing results will be stored in `./result.csv`.

### 6.2 Evaluation

#### 6.2.1 Evaluation for Ramos

**Internal variety**:

**Step 1:** Set **file_path** and **result_csv** in **./Ramos_Structure.py** and **./Ramos_Structure_and_precision_bugs.py**. Especially, set **file_path** to the location of result.csv(new generated). Set **result_csv** to the location of the structural analysis output.

**Step 2:** Use the following command:

```shell
python Ramos_Structure.py
python Ramos_Structure_and_precision_bugs.py
```

The structural analysis results will be stored `./Ramos_graph.csv` and`./Ramos_graph_and_precision_bugs.csv`.

**External variety:**

**Step 1:** Set **file_path** in **./Ramos_edit_distance**. Especially, set **file_path** to the location of result.csv(new generated).

**Step 2:** Use the following command:

```shell
python Ramo_edit_distance.py
```

#### 6.2.2 Evaluation for Cradle, Muffin and LEMON

Generate model files using open source code of their respective methods. Set the path of the corresponding decoder file and run the decoder file to convert the model to a string. Then run the corresponding analysis method. The configuration similar to 6.2.1.

## 7. API mapping rules

We conclude 15 API mappings between TensorFlow, PyTorch and MindSpore. The operator types in our API mapping rules cover all main operator type in DL framework.

1. #### BatchNorm

   | Framework  | API                                                          |
   | :--------: | :----------------------------------------------------------- |
   | TensorFlow | thismean, thisvariance = tensorflow.nn.moments(thisresult, axes=[0, 1, 2], keepdims=True)<br/> tensorflow.nn.batch\_normalization(mean=thismean, variance=thisvariance, offset=None, scale=None, variance\_epsilon=1e-5) |
   |  PyTorch   | torch.nn.BatchNorm2d(\_BatchNorm)                            |
   | MindSpore  | mindspore.nn.BatchNorm2d(num\_features=self.in\_channel,eps=1e-5, momentum=0.9, affine=True, gamma\_init="ones", beta\_init="zeros", moving\_mean\_init="zeros", moving\_var\_init="ones", use\_batch\_statistics=True, data\_format="NCHW")(thisresult) |

   

2. #### identity

   | Framework  | API  |
   | :--------: | :--- |
   | TensorFlow | None |
   |  PyTorch   | None |
   | MindSpore  | None |

3. #### Concatenate

   | Framework  | API                          |
   | :--------: | :--------------------------- |
   | TensorFlow | tensorflow.concat(axis=3)    |
   |  PyTorch   | torch.cat(dim=1)             |
   | MindSpore  | mindspore.ops.Concat(axis=1) |

4. #### 1 * 1 Convolution

   | Framework  | API                                      |
   | :--------: | :--------------------------------------- |
   | TensorFlow | tensorflow.nn.conv2d(padding$=$ ‘SAME’)  |
   |  PyTorch   | torch.nn.functional.conv2d(padding$=$ 0) |
   | MindSpore  | mindspore.nn.Conv2d(padding $=$ 0)       |

5. #### 3 * 3 Convolution

   | Framework  | API                                      |
   | :--------: | :--------------------------------------- |
   | TensorFlow | tensorflow.nn.conv2d(padding$=$‘SAME’)   |
   |  PyTorch   | torch.nn.functional.conv2d(padding$=$ 1) |
   | MindSpore  | mindspore.nn.Conv2d(padding$=$1)         |

6. #### Depthwise Convolution

   | Framework  | API                                                          |
   | :--------: | :----------------------------------------------------------- |
   | TensorFlow | tensorflow.nn.depthwise\_conv2d(padding $=$ 'SAME')          |
   |  PyTorch   | torch.nn.ZeroPad2d(padding$=$(1, 1, 1, 1))<br/>torch.nn.functional.conv2d(padding $=$ 0,groups $=$ in\_channel) |
   | MindSpore  | mindspore.nn.Conv2d(in\_channels, out\_channels, group, pad\_mode $=$ 'same') //the value of in\_channels, out\_channels, group is the same. |

7. #### Average Pooling

   | Framework  | API                                                          |
   | :--------: | :----------------------------------------------------------- |
   | TensorFlow | tensorflow.nn.avg\_pool2d(padding $=$ 'SAME')                |
   |  PyTorch   | torch.nn.AvgPool2d(padding$=$1, ceil\_mode$=$True, count\_include\_pad$=$False) |
   | MindSpore  | mindspore.nn.AvgPool2d(pad\_mode $=$ 'same')                 |

8. #### Max Pooling

   | Framework  | API                                                 |
   | :--------: | :-------------------------------------------------- |
   | TensorFlow | tensorflow.nn.max\_pool2d(padding $=$ 'SAME')       |
   |  PyTorch   | torch.nn.MaxPool2d(padding$=$1, ceil\_mode$=$False) |
   | MindSpore  | mindspore.nn.MaxPool2d(pad\_mode $=$ 'same')        |

9. #### Transpose Convolution

   | Framework  | API                                                 |
   | :--------: | :-------------------------------------------------- |
   | TensorFlow | tensorflow.nn.conv2d\_transpose(padding $=$ 'SAME') |
   |  PyTorch   | torch.nn.functional.conv\_transpose2d(padding$=$1)  |
   | MindSpore  | mindspore.nn.Conv2dTranspose(pad\_mode $=$ 'same')  |

10. #### ReLU

    | Framework  | API                      |
    | :--------: | :----------------------- |
    | TensorFlow | tensorflow.nn.relu       |
    |  PyTorch   | torch.nn.functional.relu |
    | MindSpore  | mindspore.nn.layer.ReLU  |

11. #### Sigmoid

    | Framework  | API                        |
    | :--------: | :------------------------- |
    | TensorFlow | tensorflow.nn.sigmoid      |
    |  PyTorch   | torch.sigmoid              |
    | MindSpore  | mindpsore.nn.layer.Sigmoid |

12. #### LeakyReLU

    | Framework  | API                                     |
    | :--------: | :-------------------------------------- |
    | TensorFlow | tensorflow.nn.leaky\_relu               |
    |  PyTorch   | torch.nn.LeakyReLU(negative\_slope=0.2) |
    | MindSpore  | mindspore.nn.LeakyReLU                  |

13. #### Tanh

    | Framework  | API                     |
    | :--------: | :---------------------- |
    | TensorFlow | tensorflow.nn.tanh      |
    |  PyTorch   | torch.tanh              |
    | MindSpore  | mindspore.nn.layer.Tanh |

14. #### PReLU

    | Framework  | API                |
    | :--------: | :----------------- |
    | TensorFlow | Not Support        |
    |  PyTorch   | torch.nn.PReLU     |
    | MindSpore  | mindspore.nn.PReLU |

15. #### ELU

    | Framework  | API               |
    | :--------: | :---------------- |
    | TensorFlow | tensorflow.nn.elu |
    |  PyTorch   | torch.nn.ELU      |
    | MindSpore  | mindspore.nn.ELU  |